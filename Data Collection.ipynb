{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Process for Team Vaccinated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Datasets\n",
    "\n",
    "We collected our data by directly parsing the HTML DOM of pages on Facebook.  \n",
    "\n",
    "To find the pages, we searched specific queries on Facebook's search bar: \n",
    "\n",
    "####                [\"stop vaccination\", \"anti vaccination\"], [\"parental advice\", \"child health\"]\n",
    "         \n",
    "For each search, we followed this process:\n",
    "\n",
    "1. Search the term.\n",
    "2. Click the \"Pages\" tab to filter to only Page results.\n",
    "3. Scroll downward until no results are left.  If laborious, you may use this script:\n",
    "```javascript\n",
    "var s = () => {window.scrollTo(0,document.body.scrollHeight); setTimeout(s, 500);};\n",
    "s();\n",
    "```\n",
    "3. Open up the console and input this script:\n",
    "```javascript\n",
    "var parse = () => {\n",
    "    return Array.from(document.getElementsByClassName('_3u1 _gli _6pe1')).map((elem) => {\n",
    "        var mo = elem.getElementsByClassName('_32mo')[0];\n",
    "        var name = mo.textContent;\n",
    "        var href = mo.getAttribute('href');\n",
    "        let likes = elem.getElementsByClassName('_pac')[0].getElementsByTagName('a')[0];\n",
    "        likes = (likes) ? likes.textContent.split(' ')[0] : null;\n",
    "        return {name: name, href: href, likes: likes};\n",
    "    });\n",
    "}\n",
    "copy(parse());\n",
    "```\n",
    "4. The page results are now copied to your clipboard.  Open a text editor and paste the results into a file and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Alec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data/Datasets - anti vaccination.json', 'Data/Datasets - child health.json', 'Data/Datasets - infant parent advice.json', 'Data/Datasets - stop vaccination.json']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from fbdp import FBDesktopParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "datasets = ['Data/' + f for f in os.listdir('Data/') if f.startswith('Datasets - ')]\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>likes</th>\n",
       "      <th>href</th>\n",
       "      <th>href_posts</th>\n",
       "      <th>anti_vax</th>\n",
       "      <th>likes_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#StopBullying</th>\n",
       "      <td>stop vaccination</td>\n",
       "      <td>9.8K</td>\n",
       "      <td>https://www.facebook.com/StopBullying112/?ref=...</td>\n",
       "      <td>https://www.facebook.com/pg/StopBullying112/po...</td>\n",
       "      <td>True</td>\n",
       "      <td>9800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/8 autistic meme causing anti-vaccination</th>\n",
       "      <td>anti vaccination</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.facebook.com/28-autistic-meme-caus...</td>\n",
       "      <td>https://www.facebook.com/pg/28-autistic-meme-c...</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP Section On International Child Health</th>\n",
       "      <td>child health</td>\n",
       "      <td>620</td>\n",
       "      <td>https://www.facebook.com/AAPSOICH/?ref=br_rs</td>\n",
       "      <td>https://www.facebook.com/pg/AAPSOICH/posts/?re...</td>\n",
       "      <td>False</td>\n",
       "      <td>620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APHA Maternal &amp; Child Health Section</th>\n",
       "      <td>child health</td>\n",
       "      <td>343</td>\n",
       "      <td>https://www.facebook.com/APHAMCHSection/?ref=b...</td>\n",
       "      <td>https://www.facebook.com/pg/APHAMCHSection/pos...</td>\n",
       "      <td>False</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>About Pediatrics and Parenting Advice</th>\n",
       "      <td>infant parent advice</td>\n",
       "      <td>1.5K</td>\n",
       "      <td>https://www.facebook.com/AboutPediatrics/?ref=...</td>\n",
       "      <td>https://www.facebook.com/pg/AboutPediatrics/po...</td>\n",
       "      <td>False</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           query likes  \\\n",
       "name                                                                     \n",
       "#StopBullying                                   stop vaccination  9.8K   \n",
       "2/8 autistic meme causing anti-vaccination      anti vaccination    15   \n",
       "AAP Section On International Child Health           child health   620   \n",
       "APHA Maternal & Child Health Section                child health   343   \n",
       "About Pediatrics and Parenting Advice       infant parent advice  1.5K   \n",
       "\n",
       "                                                                                         href  \\\n",
       "name                                                                                            \n",
       "#StopBullying                               https://www.facebook.com/StopBullying112/?ref=...   \n",
       "2/8 autistic meme causing anti-vaccination  https://www.facebook.com/28-autistic-meme-caus...   \n",
       "AAP Section On International Child Health        https://www.facebook.com/AAPSOICH/?ref=br_rs   \n",
       "APHA Maternal & Child Health Section        https://www.facebook.com/APHAMCHSection/?ref=b...   \n",
       "About Pediatrics and Parenting Advice       https://www.facebook.com/AboutPediatrics/?ref=...   \n",
       "\n",
       "                                                                                   href_posts  \\\n",
       "name                                                                                            \n",
       "#StopBullying                               https://www.facebook.com/pg/StopBullying112/po...   \n",
       "2/8 autistic meme causing anti-vaccination  https://www.facebook.com/pg/28-autistic-meme-c...   \n",
       "AAP Section On International Child Health   https://www.facebook.com/pg/AAPSOICH/posts/?re...   \n",
       "APHA Maternal & Child Health Section        https://www.facebook.com/pg/APHAMCHSection/pos...   \n",
       "About Pediatrics and Parenting Advice       https://www.facebook.com/pg/AboutPediatrics/po...   \n",
       "\n",
       "                                            anti_vax  likes_adj  \n",
       "name                                                             \n",
       "#StopBullying                                   True     9800.0  \n",
       "2/8 autistic meme causing anti-vaccination      True       15.0  \n",
       "AAP Section On International Child Health      False      620.0  \n",
       "APHA Maternal & Child Health Section           False      343.0  \n",
       "About Pediatrics and Parenting Advice          False     1500.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(filename, query):\n",
    "    with open(filename) as f:\n",
    "        d = pd.DataFrame(json.loads(f.read()), columns=['name', 'likes', 'href'])\n",
    "        d['query'] = query\n",
    "        return d\n",
    "    raise Exception(\"Could not open file\")\n",
    "def load_datasets(datasets):\n",
    "    data = pd.concat([load_dataset(f, q) for f, q in zip(\n",
    "        datasets, [n.split('.')[0].split('- ')[1] for n in datasets])])\n",
    "    gb = data.groupby('name')\n",
    "    base = pd.DataFrame(gb.query.apply(set).apply(list).apply(','.join))\n",
    "    base['likes'] = gb.likes.last().fillna('0')\n",
    "    base['href'] = gb.href.last()\n",
    "    base['href_posts'] = base.href.apply(lambda x: 'https://www.facebook.com/pg/' + \\\n",
    "                                         x[25:].split('/')[0] + '/posts/?ref=page_internal')\n",
    "    base['anti_vax'] = base['query'].apply(lambda x: 'vaccination' in x)\n",
    "    def convert_num(n):\n",
    "        if 'M' in n:\n",
    "            return float(n[:-1]) * 1e6\n",
    "        elif 'K' in n:\n",
    "            return float(n[:-1]) * 1e3\n",
    "        else:\n",
    "            return float(n)\n",
    "    base['likes_adj'] = base.likes.apply(convert_num)\n",
    "    return base\n",
    "data = load_datasets(datasets)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unwanted pages\n",
    "\n",
    "Given that the searches might not return results which are intended for the dataset, we must manually evaluate and filter pages. The criteria that we use is as follows:\n",
    "\n",
    "- Page must explicitly and specifically dedicate itself to expanding, supporting, or espousing either the anti-vaccination movement or child health care.\n",
    "- Page must have at least 10 posts.\n",
    "- If the page specifies a location, it must be a predominately English-speaking location.\n",
    "- Page must exclusively target the general topics of anti-vaccination or child health advice.  If the page specifically targets a subtopic, then it should be filtered out.\n",
    "- Page must target the discussion of its topic.  Example of non-qualifier: a landing page for a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child health                         34\n",
      "stop vaccination                     29\n",
      "infant parent advice                 27\n",
      "anti vaccination                     15\n",
      "stop vaccination,anti vaccination    14\n",
      "child health,infant parent advice     1\n",
      "Name: query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "stop_pages = json.loads(open('Data/stop_pages.json').read())\n",
    "for k in stop_pages.keys():\n",
    "    stop_pages[k] = set(stop_pages[k])\n",
    "def remove_unwanted_pages(df):\n",
    "    df = df.loc[(~df.index.isin(stop_pages['non_anti_vax_pages'])) & \\\n",
    "                (~df.index.isin(stop_pages['unwanted_normal_pages']))]\n",
    "    def has_banned_terms(x):\n",
    "        x = x.lower()\n",
    "        for term in stop_pages['banned_terms']:\n",
    "            if term in x:\n",
    "                return True\n",
    "        return False\n",
    "    return df[[not has_banned_terms(x) for x in df.index]]\n",
    "data = remove_unwanted_pages(data)\n",
    "print(data['query'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing each page\n",
    "\n",
    "To parse each Facebook page, we follow a set methodology:\n",
    "\n",
    "1. Head to the Posts section of the Facebook page you want to parse.\n",
    "\tThen, open up your browser's console (Chrome: Ctrl + Shift + J).\n",
    "\n",
    "2. Paste the following code into the console and press Enter:\n",
    "\n",
    "***\n",
    "```javascript\n",
    "try{document.getElementById(\"pagelet_bluebar\").remove()}catch(e){}try{document.getElementById(\"pagelet_sidebar\").remove()}catch(e){}try{document.getElementById(\"uiContextualLayerParent\").style.display=\"none\"}catch(e){}try{document.getElementsByClassName(\"_1qks\")[0].remove()}catch(e){}try{document.getElementsByClassName(\"_1pfm\")[0].remove()}catch(e){}try{document.getElementsByClassName(\"uiScrollableAreaContent\")[0].remove()}catch(e){}var prog=document.querySelector('div[role=\"contentinfo\"]');prog.style=\"position: fixed;top: 50%;right: 10%;z-index: 999;width: 200px;font-size:4.5em;text-shadow:1px 1px 4px black;\";try{prog.querySelector(\".fsm\").remove()}catch(e){}prog.textContent=\"\";var random_fun=()=>\"#\"+((1<<24)*Math.random()|0).toString(16),updated_count={posts:0,times_counted:0},update=()=>{let e=document.getElementsByClassName(\"userContentWrapper\").length;if(e==updated_count.posts){if(updated_count.times_counted++,updated_count.times_counted>=15)return console.log(\"Tried scrolling 15 times, but no update has occurred.\"),prog.textContent=\"Loaded \"+e+\" posts. Right-click and Save!\",!0}else updated_count.posts=e,updated_count.times_counted=0;return prog.textContent=\"Loaded \"+e+\" posts\",prog.style.color=random_fun(),!1},hide=()=>{Array.from(document.querySelectorAll(\".userContentWrapper:not(.done)\")).forEach(e=>{e.style.display=\"none\",e.classList.add(\"done\")})},scroll=()=>{window.scrollTo(0,document.body.scrollHeight);let e=document.querySelector(\".mhs\");e&&e.click()},complete=()=>{updated_count={posts:0,times_counted:0},Array.from(document.getElementsByClassName(\"see_more_link_inner\")).forEach(e=>e.click()),console.log(\"See More's clicked. You may now save. (Done)\")},get_abbr_year=e=>e?new Date(e.title).getFullYear():new Date,oversized_count=()=>!!(updated_count&&updated_count.posts>=1500)&&(complete(),console.log(\"Your page has become too big!  Save the page now as one chunk (name it as such) then call 'next()'.\"),!0),next=()=>{Array.from(document.getElementsByClassName(\"_4-u2\")).forEach(e=>e.remove());let e=document.querySelector(\".mhs\");e&&e.click(),console.log(\"Page Cleared. You may now call scroll_till_year or scroll_i_times to begin the next chunk.\"),scroll()},scroll_till_year=e=>{let t=Array.from(document.getElementsByClassName(\"timestampContent\")).slice(-10).map(e=>e.parentElement).filter(e=>!e.classList.contains(\"livetimestamp\")).map(get_abbr_year).filter(e=>!isNaN(e)),o=t.length?Math.max(...t):2019,l=Math.min(...t);if(!oversized_count()){if(o<=e||update())return setTimeout(complete,2e3);o!=l?console.log(\"Possible last seen:\",o):console.log(\"Last seen:\",o),scroll(),setTimeout(()=>{scroll_till_year(e)},1e3*(Math.random()+1)),hide()}},scroll_i_times=e=>{if(!oversized_count()){if(0==e||update())return setTimeout(complete,2e3);e%10==0&&console.log(\"Scrolling\",e,\"times.\"),scroll(),setTimeout(()=>{scroll_i_times(e-1)},1e3*(Math.random()+1)),hide()}};clear(),scroll_till_year(2013);\n",
    "```\n",
    "***\n",
    "The above code is the following, in a semi-readable format:\n",
    "```javascript\n",
    "try { document.getElementById('pagelet_bluebar').remove(); } catch (err) {}\n",
    "try { document.getElementById('pagelet_sidebar').remove(); } catch (err) {}\n",
    "try { document.getElementById('uiContextualLayerParent').style.display = 'none'; } catch (err) {}\n",
    "try { document.getElementsByClassName('_1qks')[0].remove(); } catch (err) {}\n",
    "try { document.getElementsByClassName('_1pfm')[0].remove(); } catch (err) {}\n",
    "try { document.getElementsByClassName('uiScrollableAreaContent')[0].remove(); } catch (err) {}\n",
    "var prog = document.querySelector('div[role=\"contentinfo\"]');\n",
    "prog.style = \"position: fixed;top: 50%;right: 10%;z-index: 999;width: 200px;font-size:4.5em;text-shadow:1px 1px 4px black;\"\n",
    "try { prog.querySelector('.fsm').remove(); } catch (err) {}\n",
    "prog.textContent = '';\n",
    "var random_fun = () => \"#\"+((1<<24)*Math.random()|0).toString(16);\n",
    "var updated_count = {posts: 0, times_counted: 0};\n",
    "var update = () => {\n",
    "    let posts = document.getElementsByClassName('userContentWrapper').length;\n",
    "    if (posts == updated_count.posts) {\n",
    "        updated_count.times_counted++;\n",
    "        if (updated_count.times_counted >= 15) {\n",
    "            console.log(\"Tried scrolling 15 times, but no update has occurred.\");\n",
    "            prog.textContent = \"Loaded \" + posts + \" posts. Right-click and Save!\"; \n",
    "            return true;\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        updated_count.posts = posts;\n",
    "        updated_count.times_counted = 0;\n",
    "    }\n",
    "    prog.textContent = \"Loaded \" + posts + \" posts\"; \n",
    "    prog.style.color = random_fun()\n",
    "    return false;\n",
    "};\n",
    "var hide = () => {\n",
    "    Array.from(document.querySelectorAll('.userContentWrapper:not(.done)')).forEach((elem) => {\n",
    "        elem.style.display = 'none';\n",
    "        elem.classList.add('done');\n",
    "    });\n",
    "}\n",
    "var scroll = (() => {\n",
    "    window.scrollTo(0, document.body.scrollHeight);\n",
    "    let see_more = document.querySelector('.mhs');\n",
    "    if (see_more) see_more.click();\n",
    "});\n",
    "var complete = (() => {\n",
    "    updated_count = {posts: 0, times_counted: 0};\n",
    "    Array.from(document.getElementsByClassName(\"see_more_link_inner\")).forEach(\n",
    "        e => e.click());\n",
    "    console.log(\"See More's clicked. You may now save. (Done)\");\n",
    "});\n",
    "var get_abbr_year = (e => e ? new Date(e.title).getFullYear() : new Date);\n",
    "var oversized_count = () => {\n",
    "    if (updated_count && updated_count.posts >= 1200) {\n",
    "        complete();\n",
    "        console.log(\"Your page has become too big!  Save the page now as one chunk (name it as such) then call 'next()'.\");\n",
    "        return true;\n",
    "    }\n",
    "    return false;\n",
    "};\n",
    "var next = () => {\n",
    "    Array.from(document.getElementsByClassName('_4-u2')).forEach(elem => elem.remove());\n",
    "    let see_more = document.querySelector('.mhs');\n",
    "    if (see_more) see_more.click();\n",
    "    console.log(\"Page Cleared. You may now call scroll_till_year or scroll_i_times to begin the next chunk.\");\n",
    "    scroll();\n",
    "};\n",
    "var scroll_till_year = (year => {\n",
    "    let found_years = Array.from(document.getElementsByClassName(\"timestampContent\")).slice(-10)\n",
    "                .map(e => e.parentElement).filter(e => !e.classList.contains(\"livetimestamp\"))\n",
    "                .map(get_abbr_year).filter(e => !isNaN(e));\n",
    "    let max_found_year = (found_years.length) ? Math.max(...found_years) : 2019;\n",
    "    let min_found_year = Math.min(...found_years);\n",
    "    if (oversized_count()) return;\n",
    "    else if (max_found_year <= year || update()) return setTimeout(complete, 2000);\n",
    "    else if (max_found_year != min_found_year) console.log(\"Possible last seen:\", max_found_year);\n",
    "    else console.log(\"Last seen:\", max_found_year);\n",
    "    scroll();\n",
    "    setTimeout(() => {\n",
    "        scroll_till_year(year)\n",
    "    }, 1000 * (Math.random() + 1));\n",
    "    hide();\n",
    "});\n",
    "var scroll_i_times = (i => {\n",
    "    if (oversized_count()) return;\n",
    "    else if (i == 0 || update()) return setTimeout(complete, 2000);\n",
    "    else if (i % 10 == 0) console.log(\"Scrolling\", i, \"times.\");\n",
    "    scroll();\n",
    "    setTimeout(() => {\n",
    "        scroll_i_times(i - 1)\n",
    "    }, 1000 * (Math.random() + 1));\n",
    "    hide();\n",
    "});\n",
    "clear();\n",
    "scroll_till_year(2013);\n",
    "```\n",
    "***\n",
    "\n",
    "3. You can now scroll for a certain number of times or until you\n",
    "\treach a certain year using:\n",
    "```javascript\n",
    "scroll_i_times(100); //Tries to scroll 100 times\n",
    "scroll_till_year(2015); //Scrolls until bottom post from 2015\n",
    "```\n",
    "4. Right click the page and click \"Save As\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Validating Loaded Files\n",
    "\n",
    "We use this section to validate the HTML data files we downloaded for completeness and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data.index.str.replace('/', '_').str.replace(':', '_').str.replace('|', '_').str.replace('?', '_')\n",
    "found_files = set(['-'.join(page.split('-')[:-1]).strip() for page in os.listdir('Data/') if page.endswith('sts.html')])\n",
    "for file in found_files:\n",
    "    assert not file.startswith('('), file + \" Should not start with (#)\"\n",
    "data['Found'] = [x in found_files for x in data.index]\n",
    "#Assert no pages have not been collected\n",
    "assert data.Found.all(), \"All files should be downloaded\"\n",
    "data.to_csv('Data_Clean/pages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed801c4600548568a4a529cc756c531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=120), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Ensure that all pages can be parsed (Can skip if already completed)\n",
    "validate = False\n",
    "for page_name in tqdm(data.index):\n",
    "    if not validate:\n",
    "        continue\n",
    "    filename = 'Data/' + page_name + ' - Posts.html'\n",
    "    parser = FBDesktopParser(filename)\n",
    "    parser.parse_posts(limit=10)\n",
    "    assert parser.posts.shape[0] == 10, page_name + \" doesn't have at least 10 posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate parser and validate data for given filename\n",
    "def generate_page_posts(file, verbose=False):\n",
    "    parser = FBDesktopParser(file)\n",
    "    #Assert parser read posts from file\n",
    "    assert parser.parse_posts().shape[0] >= 10, file + ', ' + str(parser.posts.shape[0])\n",
    "    parser.posts = parser.posts.loc[parser.posts.timestamp > '2013-12-31']\n",
    "    parser.posts = parser.posts.iloc[:1500]\n",
    "    see_more = parser.posts.text.apply(lambda x: 'See More' in x).sum()\n",
    "    assert see_more < 5, \"'See More' should not appear multiple times\"\n",
    "    if verbose:\n",
    "        print(file, 'Posts: ', parser.posts.shape[0], '- Number of See More\\'s:', see_more)\n",
    "    parser.extract_features(bag_of_words=False, lemmatize=True)\n",
    "    return parser.posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d17f7a942e4c499d8fb8855b6a824d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=120), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/About Pediatrics and Parenting Advice - Posts.html Posts:  1129 - Number of See More's: 0\n",
      "Data/Adult and Child Health - Posts.html Posts:  816 - Number of See More's: 0\n",
      "Data/Advice4Parenting - Posts.html Posts:  8 - Number of See More's: 0\n",
      "Data/Anti Vaccination Saskatoon - Posts.html Posts:  1500 - Number of See More's: 0\n",
      "Data/Anti Vaccinations - Posts.html Posts:  232 - Number of See More's: 0\n",
      "Data/Anti-Vaccination Australia - Posts.html Posts:  943 - Number of See More's: 0\n",
      "Data/Anti-Vaccination Movement UK - Posts.html Posts:  160 - Number of See More's: 0\n",
      "Data/Anti-Vaccine Choice MA - Posts.html Posts:  1500 - Number of See More's: 0\n",
      "Data/Are Vaccines Safe - Posts.html Posts:  72 - Number of See More's: 0\n",
      "Data/Association of Teachers of Maternal and Child Health - ATMCH - Posts.html Posts:  88 - Number of See More's: 0\n",
      "Data/Assuring Better Child Health & Development - Posts.html Posts:  360 - Number of See More's: 0\n",
      "Data/Australian Vaccination-risks Network Inc. - AVN - Posts.html Posts:  1500 - Number of See More's: 0\n",
      "Data/BellyBelly - Pregnancy, Birth & Parenting - Posts.html Posts:  1500 - Number of See More's: 0\n",
      "Data/Boys Town Parenting - Posts.html Posts:  1500 - Number of See More's: 0\n",
      "Data/Briar's journey after HPV vaccine injury - Posts.html Posts:  282 - Number of See More's: 0\n",
      "Data/BuzzFeed Parents - Posts.html Posts:  1500 - Number of See More's: 0\n",
      "Data/C.H.I.L.D - Posts.html Posts:  191 - Number of See More's: 0\n",
      "Data/California Anti-Vax Mommies - Posts.html Posts:  817 - Number of See More's: 0\n"
     ]
    }
   ],
   "source": [
    "#Generate all parsers\n",
    "parsers = []\n",
    "for page_name in tqdm(data.index, smoothing=0):\n",
    "    filename = 'Data/' + page_name + ' - Posts.html'\n",
    "    if not os.path.exists(filename):\n",
    "        raise Exception(\"File Not Found: \" + page_name)\n",
    "    try:\n",
    "        parser = generate_page_posts(filename, verbose=False)\n",
    "        parser['page_name'] = page_name\n",
    "        parser['anti_vax'] = data.loc[page_name].anti_vax\n",
    "        parsers.append(parser)\n",
    "    except Exception as e:\n",
    "        print(page_name, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all pages into one and save as a checkpoint\n",
    "cleaned_df = pd.concat(parsers).reset_index(drop=True)\n",
    "if not os.path.exists('Data_Clean'):\n",
    "    os.makedirs('Data_Clean')\n",
    "cleaned_df.to_csv('Data_Clean/posts.csv')\n",
    "cleaned_df.to_json('Data_Clean/posts_json.json')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove page from its own linked_profiles (faulty link)\n",
    "names = remove_unwanted_pages(load_datasets(['Data/' + f for f in os.listdir('Data/') if f.startswith('Datasets - ')]))\n",
    "names['page_name_adjusted'] = names.index.str.replace('/', '_').str.replace(':', '_').str.replace(\n",
    "                                        '|', '_').str.replace('?', '_')\n",
    "cleaned_df = cleaned_df.join(names[['page_name_adjusted']], on='page_name')\n",
    "def check_bad_links(row):\n",
    "    if len(row.linked_profiles) > 0:\n",
    "        if row.linked_profiles is not None and row.page_name in row.linked_profiles:\n",
    "            return row.linked_profiles.remove(row.page_name)\n",
    "        if row.linked_profiles is not None and row.page_name_adjusted in row.linked_profiles:\n",
    "            return row.linked_profiles.remove(row.page_name_adjusted) \n",
    "    return row.linked_profiles\n",
    "cleaned_df['linked_profiles'] = cleaned_df.apply(check_bad_links, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change objects/lists into seperate fields\n",
    "for sen in ['neg', 'neu', 'pos', 'compound']:\n",
    "    cleaned_df['sentiment_' + sen] = cleaned_df.sentiment.apply(lambda x: x[sen])\n",
    "cleaned_df.drop('sentiment', axis=1, inplace=True)\n",
    "for i, r in enumerate(['smog_index', 'gunning_fog', 'flesch_kincaid_grade']):\n",
    "    cleaned_df['readability_' + r] = cleaned_df.readability.apply(lambda x: x[i])\n",
    "cleaned_df.drop('readability', axis=1, inplace=True)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add LIWC and other features\n",
    "#from Liwc import LiwcAnalyzer\n",
    "#liwc = LiwcAnalyzer()\n",
    "#liwc_results = liwc.parse(cleaned_df.text)\n",
    "#cleaned_df = cleaned_df.join(liwc_results.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('Data_Clean/posts.csv')\n",
    "cleaned_df.to_json('Data_Clean/posts_json.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
